#!/usr/bin/env python3
"""
Step 3: Generate 2019 Signals (Fixed Version)
Uses the trained models to generate buy signals for January 2019
"""

import pandas as pd
import numpy as np
import os
import pickle
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

class Signal2019GeneratorFixed:
    def __init__(self):
        self.testing_data_dir = "testing_data_2019_2025"
        self.models_dir = "trained_models_2019"
        self.signals_dir = "signals_2019"
        
        # Create signals directory
        os.makedirs(self.signals_dir, exist_ok=True)
        
        # Load trained models
        self.models = {}
        self.scaler = None
        self.feature_columns = []
        
    def load_trained_models(self):
        """Load the trained ML models"""
        print("ü§ñ Loading trained ML models...")
        
        try:
            # Load models
            model_files = {
                'RandomForest': 'RandomForest_multibagger_2019.pkl',
                'GradientBoosting': 'GradientBoosting_multibagger_2019.pkl',
                'LogisticRegression': 'LogisticRegression_multibagger_2019.pkl'
            }
            
            for name, filename in model_files.items():
                model_path = f"{self.models_dir}/{filename}"
                if os.path.exists(model_path):
                    with open(model_path, 'rb') as f:
                        self.models[name] = pickle.load(f)
                    print(f"‚úÖ Loaded {name}")
                    
            # Load scaler
            scaler_path = f"{self.models_dir}/scaler_2019.pkl"
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                print("‚úÖ Loaded scaler")
                
            # Load metadata
            metadata_path = f"{self.models_dir}/training_metadata.pkl"
            if os.path.exists(metadata_path):
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                    self.feature_columns = metadata['feature_columns']
                print("‚úÖ Loaded feature columns")
                
            return len(self.models) > 0
            
        except Exception as e:
            print(f"‚ùå Error loading models: {str(e)}")
            return False
            
    def load_early_2019_data(self):
        """Load stock data for early 2019 (first few months)"""
        print("üìä Loading early 2019 data for signal generation...")
        
        testing_files = [f for f in os.listdir(self.testing_data_dir) if f.endswith('.csv')]
        print(f"üìÅ Processing {len(testing_files)} testing files")
        
        early_2019_data = {}
        
        for file in testing_files:
            try:
                # Load stock data
                df = pd.read_csv(f"{self.testing_data_dir}/{file}")
                df['Date'] = pd.to_datetime(df['Date'])
                df.set_index('Date', inplace=True)
                
                # Get first 3 months of 2019 for better technical indicator calculation
                early_2019 = df[(df.index >= '2019-01-01') & (df.index <= '2019-03-31')]
                
                if len(early_2019) >= 20:  # Need at least 20 days for indicators
                    symbol = file.replace('_testing.csv', '')
                    early_2019_data[symbol] = early_2019
                    
            except Exception as e:
                print(f"‚ùå Error processing {file}: {str(e)}")
                continue
                
        print(f"‚úÖ Loaded early 2019 data for {len(early_2019_data)} stocks")
        return early_2019_data
        
    def calculate_technical_indicators(self, df):
        """Calculate technical indicators with proper error handling"""
        try:
            # Ensure we have enough data
            if len(df) < 50:
                return None
                
            # Price-based indicators
            df['SMA_20'] = df['Close'].rolling(20, min_periods=10).mean()
            df['SMA_50'] = df['Close'].rolling(50, min_periods=25).mean()
            df['EMA_12'] = df['Close'].ewm(span=12, min_periods=6).mean()
            df['EMA_26'] = df['Close'].ewm(span=26, min_periods=13).mean()
            
            # RSI
            delta = df['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=7).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=7).mean()
            rs = gain / loss
            df['RSI'] = 100 - (100 / (1 + rs))
            
            # MACD
            df['MACD'] = df['EMA_12'] - df['EMA_26']
            df['MACD_Signal'] = df['MACD'].ewm(span=9, min_periods=5).mean()
            df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']
            
            # Bollinger Bands
            df['BB_Middle'] = df['Close'].rolling(20, min_periods=10).mean()
            bb_std = df['Close'].rolling(20, min_periods=10).std()
            df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)
            df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)
            
            # Avoid division by zero
            bb_range = df['BB_Upper'] - df['BB_Lower']
            df['BB_Position'] = np.where(bb_range > 0, 
                                       (df['Close'] - df['BB_Lower']) / bb_range, 
                                       0.5)
            
            # Volume indicators
            df['Volume_SMA'] = df['Volume'].rolling(20, min_periods=10).mean()
            df['Volume_Ratio'] = np.where(df['Volume_SMA'] > 0, 
                                        df['Volume'] / df['Volume_SMA'], 
                                        1.0)
            
            # Price momentum
            df['Price_Change_1D'] = df['Close'].pct_change(1)
            df['Price_Change_5D'] = df['Close'].pct_change(5)
            df['Price_Change_20D'] = df['Close'].pct_change(20)
            
            # Volatility
            df['Volatility_20D'] = df['Close'].pct_change().rolling(20, min_periods=10).std()
            
            # Trend strength
            df['Trend_Strength'] = np.where(df['SMA_50'] > 0, 
                                          (df['Close'] - df['SMA_50']) / df['SMA_50'], 
                                          0)
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error calculating indicators: {str(e)}")
            return None
            
    def generate_ensemble_predictions(self, features):
        """Generate predictions using ensemble of models"""
        predictions = {}
        
        # Get predictions from each model
        for name, model in self.models.items():
            try:
                if name == 'LogisticRegression':
                    # Use scaled features for logistic regression
                    scaled_features = self.scaler.transform(features)
                    prob = model.predict_proba(scaled_features)[:, 1]
                else:
                    # Use raw features for tree-based models
                    prob = model.predict_proba(features)[:, 1]
                    
                predictions[name] = prob
            except Exception as e:
                print(f"‚ùå Error with {name}: {str(e)}")
                predictions[name] = np.array([0.5])  # Default probability
                
        # Create ensemble prediction (weighted average)
        ensemble_weights = {
            'GradientBoosting': 0.5,  # Best model gets highest weight
            'RandomForest': 0.3,
            'LogisticRegression': 0.2
        }
        
        ensemble_prob = np.zeros(len(features))
        total_weight = 0
        
        for name, weight in ensemble_weights.items():
            if name in predictions:
                ensemble_prob += predictions[name] * weight
                total_weight += weight
                
        if total_weight > 0:
            ensemble_prob /= total_weight
            
        return ensemble_prob, predictions
        
    def generate_2019_signals(self, early_2019_data):
        """Generate buy signals for early 2019"""
        print("üéØ Generating 2019 buy signals...")
        
        signals = []
        processed = 0
        
        for symbol, data in early_2019_data.items():
            try:
                processed += 1
                if processed % 25 == 0:
                    print(f"Processed {processed}/{len(early_2019_data)} stocks...")
                
                # Calculate technical indicators
                data_with_indicators = self.calculate_technical_indicators(data)
                
                if data_with_indicators is None:
                    continue
                    
                # Get data from end of January 2019 (around 30th day)
                valid_data = data_with_indicators.dropna(subset=self.feature_columns)
                
                if len(valid_data) == 0:
                    continue
                    
                # Use data from around end of January for signal generation
                signal_date_idx = min(30, len(valid_data) - 1)  # Around Jan 30th or last available
                signal_row = valid_data.iloc[signal_date_idx]
                
                # Prepare features
                feature_values = []
                for col in self.feature_columns:
                    if col in signal_row and not pd.isna(signal_row[col]):
                        feature_values.append(signal_row[col])
                    else:
                        # Use default values for missing features
                        defaults = {
                            'RSI': 50, 'MACD': 0, 'MACD_Histogram': 0, 'BB_Position': 0.5,
                            'Volume_Ratio': 1.0, 'Price_Change_1D': 0, 'Price_Change_5D': 0,
                            'Price_Change_20D': 0, 'Volatility_20D': 0.02, 'Trend_Strength': 0
                        }
                        feature_values.append(defaults.get(col, 0))
                        
                features = np.array(feature_values).reshape(1, -1)
                
                # Generate ensemble prediction
                ensemble_prob, individual_probs = self.generate_ensemble_predictions(features)
                
                # Create signal if probability > 65% (lowered threshold to get more signals)
                if ensemble_prob[0] > 0.65:
                    signal = {
                        'symbol': symbol,
                        'date': signal_row.name.strftime('%Y-%m-%d'),
                        'price': float(signal_row['Close']),
                        'ensemble_probability': float(ensemble_prob[0]),
                        'gradient_boosting_prob': float(individual_probs.get('GradientBoosting', [0])[0]),
                        'random_forest_prob': float(individual_probs.get('RandomForest', [0])[0]),
                        'logistic_regression_prob': float(individual_probs.get('LogisticRegression', [0])[0]),
                        'rsi': float(signal_row.get('RSI', 50)),
                        'macd': float(signal_row.get('MACD', 0)),
                        'volume_ratio': float(signal_row.get('Volume_Ratio', 1.0)),
                        'trend_strength': float(signal_row.get('Trend_Strength', 0))
                    }
                    signals.append(signal)
                    
            except Exception as e:
                print(f"‚ùå Error generating signal for {symbol}: {str(e)}")
                continue
                
        # Sort signals by ensemble probability (highest first)
        signals.sort(key=lambda x: x['ensemble_probability'], reverse=True)
        
        print(f"‚úÖ Generated {len(signals)} high-confidence signals")
        return signals
        
    def save_signals(self, signals):
        """Save generated signals"""
        print("üíæ Saving 2019 signals...")
        
        # Save as JSON
        signals_file = f"{self.signals_dir}/signals_january_2019.json"
        with open(signals_file, 'w') as f:
            json.dump(signals, f, indent=2, default=str)
        print(f"‚úÖ Saved signals to {signals_file}")
        
        # Save as CSV for easy viewing
        if signals:
            df = pd.DataFrame(signals)
            csv_file = f"{self.signals_dir}/signals_january_2019.csv"
            df.to_csv(csv_file, index=False)
            print(f"‚úÖ Saved signals to {csv_file}")
            
        return signals_file
        
    def generate_signals_report(self, signals):
        """Generate comprehensive signals report"""
        
        if not signals:
            return "‚ùå No signals generated - try lowering probability threshold"
            
        # Top signals
        top_signals = signals[:15]  # Show top 15
        
        report = f"""
üéØ 2019 MULTIBAGGER SIGNALS REPORT
==================================

üìä SIGNAL SUMMARY
- Total High-Confidence Signals: {len(signals)}
- Probability Threshold: >65%
- Signal Period: January-February 2019
- Based on 2014-2019 ML Training (86.5% accuracy)

üèÜ TOP {len(top_signals)} SIGNALS (Highest Probability)
"""
        
        for i, signal in enumerate(top_signals, 1):
            report += f"""
{i:2d}. {signal['symbol']:<12} | Prob: {signal['ensemble_probability']:>6.1%} | Price: ‚Çπ{signal['price']:>8.2f} | Date: {signal['date']}
    RSI: {signal['rsi']:>5.1f} | MACD: {signal['macd']:>7.3f} | Volume: {signal['volume_ratio']:>5.2f}x | Trend: {signal['trend_strength']:>6.3f}
"""
        
        # Probability distribution
        prob_ranges = {
            '85-100%': len([s for s in signals if s['ensemble_probability'] >= 0.85]),
            '75-85%': len([s for s in signals if 0.75 <= s['ensemble_probability'] < 0.85]),
            '65-75%': len([s for s in signals if 0.65 <= s['ensemble_probability'] < 0.75])
        }
        
        report += f"""
üìà PROBABILITY DISTRIBUTION
- 85-100%: {prob_ranges['85-100%']} signals (Very High Confidence)
- 75-85%:  {prob_ranges['75-85%']} signals (High Confidence)  
- 65-75%:  {prob_ranges['65-75%']} signals (Good Confidence)

üéØ NEXT STEPS
1. Track performance of these {len(signals)} signals from 2019-2025
2. Calculate actual returns achieved by each stock
3. Identify which became 2x, 5x, 10x multibaggers
4. Validate ML model accuracy for production use

üìÅ FILES SAVED
- signals_2019/signals_january_2019.json
- signals_2019/signals_january_2019.csv

üöÄ Ready for multibagger validation!
"""
        
        return report
        
    def run_signal_generation(self):
        """Run complete signal generation process"""
        print("üöÄ Starting 2019 Signal Generation (Fixed)")
        print("=" * 50)
        
        # Step 1: Load trained models
        if not self.load_trained_models():
            print("‚ùå Failed to load trained models")
            return False
            
        # Step 2: Load early 2019 data
        early_2019_data = self.load_early_2019_data()
        if not early_2019_data:
            print("‚ùå Failed to load early 2019 data")
            return False
            
        # Step 3: Generate signals
        signals = self.generate_2019_signals(early_2019_data)
        
        # Step 4: Save signals
        signals_file = self.save_signals(signals)
        
        # Step 5: Generate report
        report = self.generate_signals_report(signals)
        
        print("\n" + "=" * 50)
        print(report)
        print("=" * 50)
        
        # Save report
        report_file = f"signals_report_2019_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w') as f:
            f.write(report)
            
        print(f"\n‚úÖ SIGNAL GENERATION COMPLETED!")
        print(f"üìä Generated {len(signals)} high-confidence signals")
        print(f"üìÅ Files saved in {self.signals_dir}/")
        
        return len(signals) > 0

if __name__ == "__main__":
    generator = Signal2019GeneratorFixed()
    success = generator.run_signal_generation()
    
    if success:
        print("\nüöÄ Next: Run step4_validate_multibaggers.py")
    else:
        print("\n‚ùå Signal generation failed. Check models and data.")
